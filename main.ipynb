{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon MPS\n"
     ]
    }
   ],
   "source": [
    "# Create a config file for the cup shuffling task\n",
    "@dataclass\n",
    "class MASTER_CONFIG:\n",
    "    # Training\n",
    "    seed: int = 1337\n",
    "    batch_size: int = 32\n",
    "    training_split: float = 0.8\n",
    "    \n",
    "    max_iters = 100\n",
    "    eval_interval = 10\n",
    "    learning_rate = 2e-5\n",
    "    eval_iters = 50\n",
    "    \n",
    "    # Model parameters\n",
    "    dtype: torch.dtype = torch.float32\n",
    "    d_model: int = 128 # This is the size of the embedding\n",
    "    l_max: int = 128 # Max sequence length\n",
    "    n_heads: int = 8 # Number of heads in the multi-head attention\n",
    "    n_layers: int = 8 # Number of layers in the transformer of MHA blocks\n",
    "    dropout: float = 0.1 # Dropout rate\n",
    "    \n",
    "    # Data parameters\n",
    "    n_cups: int = 3\n",
    "    n_moves: int = 3\n",
    "    n_samples: int = 1000\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab: list[str] = field(default_factory = lambda: [])\n",
    "    vocab_size: int = 0\n",
    "    \n",
    "    # Use CUDA or MPS if available else CPU\n",
    "    if (torch.cuda.is_available()):\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA\")\n",
    "    elif (torch.backends.mps.is_available()):\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple Silicon MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ball Shuffler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def initial_ball_position(n=3):\n",
    "    return random.randint(1, n)\n",
    "\n",
    "def generate_shuffle_moves(n=3, num_moves=3):\n",
    "    moves = []\n",
    "    \n",
    "    for _ in range(num_moves):\n",
    "        # Randomly pick two different cups\n",
    "        cup1, cup2 = random.sample(range(1, n + 1), 2)\n",
    "        moves.append((cup1, cup2))\n",
    "    \n",
    "    return moves\n",
    "\n",
    "def final_ball_position(initial_position, shuffle_moves):\n",
    "    position = initial_position\n",
    "    for move in shuffle_moves:\n",
    "        # If the ball's current position matches one of the cups in the move, swap it.\n",
    "        if position == move[0]:\n",
    "            position = move[1]\n",
    "        elif position == move[1]:\n",
    "            position = move[0]\n",
    "    \n",
    "    return position\n",
    "\n",
    "# Method for generating data and labels for batches.\n",
    "# TODO: This can be done much better\n",
    "def generate_batch(split: str, config=MASTER_CONFIG):\n",
    "    B = config.batch_size\n",
    "    T = config.l_max\n",
    "    vocab_size = config.vocab_size\n",
    "    \n",
    "    # Generate the data\n",
    "    data = generate_cup_data(config.n_cups, config.n_moves, config.n_samples)\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    if split == \"train\":\n",
    "        data = data[:int(config.training_split * len(data))]\n",
    "    elif split == \"val\":\n",
    "        data = data[int(config.training_split * len(data)):]\n",
    "    else:\n",
    "        raise ValueError(\"split must be either 'train' or 'val'\")\n",
    "    \n",
    "    # Return input and output tensors\n",
    "    inputs = torch.zeros((B, T), dtype=torch.long)\n",
    "    outputs = torch.zeros((B, T), dtype=torch.long)\n",
    "    \n",
    "    \n",
    "\n",
    "def generate_cup_shuffling_scenario(n=3, num_moves=3):\n",
    "    # Generate initial ball position and shuffle moves\n",
    "    initial_position = initial_ball_position(n)\n",
    "    shuffle_moves = generate_shuffle_moves(n, num_moves)\n",
    "    \n",
    "    # Calculate the final ball position\n",
    "    final_position = final_ball_position(initial_position, shuffle_moves)\n",
    "    \n",
    "    # Construct the input and output strings\n",
    "    str = f\"There are {n} cups\\n\"\n",
    "    str += f\"Ball is in cup {initial_position}\\n\"\n",
    "    str += \"\\n\".join([f\"Switch cup {move[0]} and cup {move[1]}\" for move in shuffle_moves])\n",
    "    str += f\"\\nBall is now in cup {final_position}\"\n",
    "    \n",
    "    return str\n",
    "\n",
    "# Generate batches of data\n",
    "def generate_cup_data(n_cups = 3, num_examples=1000, num_moves=range(1, 5)):\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        moves = random.choice(range(num_examples))\n",
    "        input = generate_cup_shuffling_scenario(n_cups, moves)\n",
    "\n",
    "        # Finds the last cup number in the string\n",
    "        output = input.split()[-1][:-5]\n",
    "        output = f\" cup {output}\"\n",
    "        \n",
    "        data.append((input, output))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[(1, 2), (3, 1), (1, 2)]\n",
      "3\n",
      "There are 3 cups\n",
      "Ball is in cup 2\n",
      "Switch cup 2 and cup 1\n",
      "Switch cup 3 and cup 1\n",
      "Switch cup 2 and cup 3\n",
      "Ball is now in cup 2\n",
      "[('There are 3 cups\\nBall is in cup 2\\n\\nBall is now in cup 2', ' cup ')]\n"
     ]
    }
   ],
   "source": [
    "initial_position = initial_ball_position()\n",
    "print(initial_position)\n",
    "\n",
    "shuffle_moves = generate_shuffle_moves()\n",
    "print(shuffle_moves)\n",
    "\n",
    "final_position = final_ball_position(initial_position, shuffle_moves)\n",
    "print(final_position)\n",
    "\n",
    "scenario = generate_cup_shuffling_scenario()\n",
    "print(scenario)\n",
    "\n",
    "print(generate_cup_data(n_cups=3, num_moves=range(1,5), num_examples=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The string is 48 tokens long:\n",
      "\n",
      "There are 4 cups\n",
      "Ball is in cup 3\n",
      "Switch cup 4 and cup 1\n",
      "Switch cup 2 and cup 4\n",
      "Switch cup 4 and cup 2\n",
      "Switch cup 3 and cup 4\n",
      "Switch cup 1 and cup 2\n",
      "Ball is now in cup 4\n",
      "\n",
      "And has the following tokenization:\n",
      "[4, 13, 19, 2, 3, 6, 9, 12, 2, 7, 9, 13, 8, 9, 10, 2, 7, 9, 11, 8, 9, 13, 2, 7, 9, 13, 8, 9, 11, 2, 7, 9, 12, 8, 9, 13, 2, 7, 9, 10, 8, 9, 11, 2, 3, 5, 9, 13]\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer, we want BOS, EOS tokens\n",
    "vocab = ['<BOS>', '<EOS>', '\\n', 'Ball', 'There are', ' is now in', ' is in', 'Switch', ' and', ' cup', ' 1', ' 2', ' 3', ' 4', ' 5', ' 6', ' 7', ' 8', ' 9', ' cups']\n",
    "\n",
    "setattr(MASTER_CONFIG, \"vocab\", vocab)\n",
    "setattr(MASTER_CONFIG, \"vocab_size\", len(vocab))\n",
    "\n",
    "idx_to_s = {i:ch for i, ch in enumerate(vocab)}\n",
    "s_to_idx = {ch:i for i, ch in enumerate(vocab)}\n",
    "\n",
    "def encode_tokens(s: str) -> list[int]:\n",
    "    ids = []\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        max_len = -1\n",
    "        max_token = None\n",
    "        for token in s_to_idx.keys():\n",
    "            token_len = len(token)\n",
    "            if s[i:i+token_len] == token:\n",
    "                if token_len > max_len:\n",
    "                    max_len = token_len\n",
    "                    max_token = token\n",
    "        if max_token:\n",
    "            ids.append(s_to_idx[max_token])\n",
    "            i += max_len\n",
    "        else:\n",
    "            print(f\"Unrecognized sequence at index {i}, {s[i:i+1]}\")\n",
    "            \n",
    "            break\n",
    "    return ids\n",
    "\n",
    "def decode_tokens(ids: list[int]) -> str:\n",
    "    return \"\".join([idx_to_s[i] for i in ids])\n",
    "\n",
    "s = generate_cup_shuffling_scenario(4, 5)\n",
    "\n",
    "print(f\"The string is {len(encode_tokens(s))} tokens long:\\n\\n{s}\\n\\nAnd has the following tokenization:\\n{encode_tokens(s)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models, functions and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Dummy model for testing purposes. It takes a sequence of tokens and returns a probability distribution over the vocabulary.\n",
    "    \n",
    "    Input: x a sequence of tokens of shape (B, T)\n",
    "    Output: a probability distribution over the vocabulary of shape (B, T, vocab_size)\n",
    "    Parameters: \n",
    "    \"\"\"\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        vocab_size = config.vocab_size\n",
    "        d_model = config.d_model\n",
    "        \n",
    "        # Embedding layer from the vocabulary to the d_model dimension, parameter matrix W_e in R^{vocab_size x d_model}\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # Linear layer from d_model to vocab_size dimension. Parameter matrices W_l1, W_l2 in R^{d_model x d_model}, R^{d_model x vocab_size}\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, vocab_size)\n",
    "        )\n",
    "        \n",
    "        # Returns the number of parameters in the model\n",
    "        self.num_parameters = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        # x is of shape (B, T), each element is an integer in the range [0, vocab_size) representing a token. By seeing each token as a one-hot vector, x can be seen as a tensor of shape (B, T, vocab_size)\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Embedd x into the d_model dimension which makes x of shape (B, T, d_model)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # MLP layer into an unembedding layer, which makes x of shape (B, T, vocab_size)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Softmax over the vocabulary dimension which is the last dimension, returns a probability distribution over the vocabulary, it is of shape (B, T, vocab_size)\n",
    "        logits = F.softmax(x, dim=-1)\n",
    "        \n",
    "        if targets is not None:\n",
    "            # Calculate the loss\n",
    "            \n",
    "            # Flatten the logits and targets to be of shape (B*T, vocab_size) and (B*T) respectively\n",
    "            x = x.view(B*T, self.vocab_size)\n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return loss, logits\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positional_encoding(nn.Module):\n",
    "    \"\"\" \n",
    "    Positional encoding according to [VSP17] paper \"Attention is all you need\" based on sine and cosine functions.\n",
    "    \n",
    "    Input: x a sequence of tokens of shape (B, T, d_model)\n",
    "    Output: p, where p is the positional encoding, of shape (B, T, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        d_model = config.d_model\n",
    "        l_max = config.l_max\n",
    "        dtype = config.dtype\n",
    "        \n",
    "        self.p = torch.zeros((1, l_max, d_model)) #TODO: Is this really necessary? Should this be l_max, d_model instead?\n",
    "        \n",
    "        # Creates X = [[0], [1], ..., [l_max - 1]]\n",
    "        num = torch.arange(l_max, dtype=dtype).reshape(-1, 1)\n",
    "        # Creates Y = [10000^0/d_model, 10000^2/d_model, ..., 10000^(d_model - 1)/d_model]\n",
    "        denum = torch.pow(10000, torch.arange(0, d_model, 2, dtype=dtype) / d_model)\n",
    "        \n",
    "        self.p[:, :, 0::2] = torch.sin(num / denum)\n",
    "        self.p[:, :, 1::2] = torch.cos(num / denum)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.p[:, :x.shape[1], :].to(x.device)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        d_model = config.d_model\n",
    "        \n",
    "        # Linear layers for Q, K, V of dimensions dq, dk, dv respectively\n",
    "        # TODO: More efficient to do the linear transformation together and then split the result?\n",
    "        # TODO: Allow a toggle for whether to use bias or not\n",
    "        self.linear_q = nn.Linear(d_model, d_model) \n",
    "        self.linear_k = nn.Linear(d_model, d_model)  \n",
    "        self.linear_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Y_t = att(X_t W_h^Q, X_t W_h^K, X_t W_h^V) = softmax((X_t W_h^Q)(X_t W_h^K)^t / sqrt(d_model)) (X_t W_h^V)\n",
    "        \n",
    "        # Linear layer for the output of the attention mechanism\n",
    "        self.linear_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # x is of shape (B, T, d_model)\n",
    "        B, T, D = x.shape\n",
    "        \n",
    "        q = self.linear_q(x) # (B, T, d_model)\n",
    "        k = self.linear_k(x) # (B, T, d_model)\n",
    "        v = self.linear_v(x) # (B, T, d_model)\n",
    "        \n",
    "        # (B, T, d_model) * (B, d_model, T) = (B, T, T)\n",
    "        if mask == None:\n",
    "            weights = torch.bmm(q, k.transpose(1, 2)) / np.sqrt(D)\n",
    "        else:\n",
    "            # TODO: Implement masking\n",
    "            pass\n",
    "\n",
    "        # Apply the softmax on the last dimension, meaning the last dimension sums to 1\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        \n",
    "        # Apply the attention weights to the values\n",
    "        # (B, T, T) * (B, T, d_model) = (B, T, d_model)\n",
    "        v_bar = torch.bmm(weights, v)\n",
    "        \n",
    "        return v_bar\n",
    "        \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        d_model = config.d_model\n",
    "        n_heads = config.n_heads\n",
    "        \n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        head_size = d_model // n_heads\n",
    "        \n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(config) for _ in range(n_heads)]\n",
    "        )\n",
    "        \n",
    "        self.linear_o = nn.Linear(head_size * n_heads, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape (B, T, d_model)\n",
    "        B, T, D = x.shape\n",
    "        H = self.config.n_heads\n",
    "        \n",
    "        # Reshape x to (B, T, H, W)\n",
    "        x = x.view(B, T, H, -1)\n",
    "        \n",
    "        # Transpose to get shape (B, H, T, W)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Apply attention heads to shape (B, H, T, W)\n",
    "        v = [head(x[:, i, :, :]) for i, head in enumerate(self.heads)] \n",
    "        # TODO: Something wrong here with tensor dimensions\n",
    "        \n",
    "        # Stack and reshape\n",
    "        v = torch.stack(v, dim=1)\n",
    "        v = v.transpose(1, 2).contiguous().view(B, T, -1)\n",
    "        \n",
    "        # Apply the linear layer\n",
    "        v_bar = self.linear_o(v)\n",
    "        \n",
    "        return v_bar\n",
    "    \n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.ln_mha = nn.LayerNorm(config.d_model)\n",
    "        self.mha = MultiHeadAttention(config)\n",
    "        \n",
    "        self.fcn = nn.Sequential(\n",
    "            nn.LayerNorm(config.d_model),\n",
    "            nn.Linear(config.d_model, config.d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.d_model, config.d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape (B, T, d_model)\n",
    "        \n",
    "        x = self.ln_mha(x)\n",
    "        x = self.mha(x) + x\n",
    "        x = self.fcn(x) + x\n",
    "        \n",
    "        return x\n",
    "\n",
    "class cup_GPT(nn.Module):\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.embed = nn.Embedding(config.vocab_size, config.d_model)\n",
    "        self.pos_enc = positional_encoding(config)\n",
    "        \n",
    "        self.mha_layers = nn.ModuleList(\n",
    "            [MultiHeadAttentionLayer(config) for _ in range(config.n_layers)]\n",
    "        )\n",
    "        \n",
    "        self.ln = nn.LayerNorm(config.d_model)\n",
    "        self.unembed = nn.Linear(config.d_model, config.vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape (B, T)\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Embed the tokens (B, T) -> (B, T, d_model)\n",
    "        x = self.embed(x)\n",
    "        \n",
    "        # Add the positional encoding (B, T, d_model) -> (B, T, d_model)\n",
    "        x = x + self.pos_enc(x)\n",
    "        \n",
    "        # Apply the MHA layers (B, T, d_model) -> (B, T, d_model)\n",
    "        for mha_layer in self.mha_layers:\n",
    "            x = mha_layer(x)\n",
    "        \n",
    "        # Apply the layer norm (B, T, d_model) -> (B, T, d_model)\n",
    "        x = self.ln(x)\n",
    "        \n",
    "        # Apply the unembedding layer (B, T, d_model) -> (B, T, vocab_size)\n",
    "        x = self.unembed(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# TODO: Next steps are to implement the decoder part of the transformer.\n",
    "# Implement:\n",
    "# - Attention mechanism\n",
    "# - Multi-head attention\n",
    "# - Layer norm\n",
    "# - Feed forward network\n",
    "# - Blocks\n",
    "# - Training loop\n",
    "# TODO: residual pass implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of DummyModel\n",
    "def train(model, optimizer, config=MASTER_CONFIG):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Generate a batch of data\n",
    "    inputs, targets = generate_batch(\"train\", config)\n",
    "    \n",
    "    # Forward pass\n",
    "    loss, logits = model(inputs, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing dummy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "tensor([[11,  7, 19]]) tensor([[[0.0532, 0.0408, 0.0340, 0.0502, 0.0486, 0.0626, 0.0407, 0.0346,\n",
      "          0.0560, 0.0509, 0.0487, 0.0771, 0.0573, 0.0519, 0.0736, 0.0322,\n",
      "          0.0399, 0.0469, 0.0320, 0.0688],\n",
      "         [0.0432, 0.0496, 0.0334, 0.0584, 0.0421, 0.0424, 0.0454, 0.0542,\n",
      "          0.0681, 0.0495, 0.0528, 0.0565, 0.0528, 0.0433, 0.0530, 0.0358,\n",
      "          0.0627, 0.0534, 0.0542, 0.0492],\n",
      "         [0.0487, 0.0634, 0.0366, 0.0467, 0.0568, 0.0563, 0.0360, 0.0572,\n",
      "          0.0584, 0.0482, 0.0527, 0.0397, 0.0329, 0.0602, 0.0550, 0.0494,\n",
      "          0.0542, 0.0473, 0.0347, 0.0654]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "config = MASTER_CONFIG\n",
    "data = generate_cup_data(config.n_cups, config.n_moves, config.n_samples)\n",
    "\n",
    "model = DummyModel(config)\n",
    "#model.to(config.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "print(MASTER_CONFIG.vocab_size)\n",
    "\n",
    "# Test model\n",
    "x = torch.randint(MASTER_CONFIG.vocab_size, (1, 3))\n",
    "y = model(x)\n",
    "\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x16 and 128x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/lejoon/Projects/cup-transformer/main.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Test model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(MASTER_CONFIG\u001b[39m.\u001b[39mvocab_size, (\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m y \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(x, y)\n",
      "File \u001b[0;32m~/Projects/cup-transformer/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/lejoon/Projects/cup-transformer/main.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m \u001b[39m# Apply the MHA layers (B, T, d_model) -> (B, T, d_model)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m \u001b[39mfor\u001b[39;00m mha_layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmha_layers:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m     x \u001b[39m=\u001b[39m mha_layer(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m \u001b[39m# Apply the layer norm (B, T, d_model) -> (B, T, d_model)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln(x)\n",
      "File \u001b[0;32m~/Projects/cup-transformer/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/lejoon/Projects/cup-transformer/main.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=126'>127</a>\u001b[0m     \u001b[39m# x is of shape (B, T, d_model)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_mha(x)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmha(x) \u001b[39m+\u001b[39m x\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfcn(x) \u001b[39m+\u001b[39m x\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Projects/cup-transformer/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/lejoon/Projects/cup-transformer/main.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m# Apply attention heads to shape (B, H, T, W)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m v \u001b[39m=\u001b[39m [head(x[:, i, :, :]) \u001b[39mfor\u001b[39;00m i, head \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads)]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39m# Stack and reshape\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(v, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/Users/lejoon/Projects/cup-transformer/main.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39m# Apply attention heads to shape (B, H, T, W)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m v \u001b[39m=\u001b[39m [head(x[:, i, :, :]) \u001b[39mfor\u001b[39;00m i, head \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads)]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39m# Stack and reshape\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(v, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/cup-transformer/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/lejoon/Projects/cup-transformer/main.ipynb Cell 16\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# x is of shape (B, T, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     B, T, D \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_q(x) \u001b[39m# (B, T, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_k(x) \u001b[39m# (B, T, d_model)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_v(x) \u001b[39m# (B, T, d_model)\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/cup-transformer/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/cup-transformer/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x16 and 128x128)"
     ]
    }
   ],
   "source": [
    "config = MASTER_CONFIG\n",
    "data = generate_cup_data(config.n_cups, config.n_moves, config.n_samples)\n",
    "\n",
    "model = cup_GPT(config)\n",
    "#model.to(config.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "print(MASTER_CONFIG.vocab_size)\n",
    "\n",
    "# Test model\n",
    "x = torch.randint(MASTER_CONFIG.vocab_size, (1, 3))\n",
    "y = model(x)\n",
    "\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-regressive test passed\n"
     ]
    }
   ],
   "source": [
    "def test_auto_regressive(model, config=MASTER_CONFIG):\n",
    "    '''\n",
    "    Tests if the model is auto-regressive by comparing the output of the model when given the entire input sequence and when given the input sequence one token at a time.\n",
    "    '''\n",
    "    \n",
    "    # shape vocab_size, batch_size, block_size\n",
    "    x = torch.randint(config.vocab_size, (1, 3))\n",
    "    y1 = model(x)\n",
    "    \n",
    "    y2 = torch.zeros_like(y1)\n",
    "    for b in range(x.size(1)):\n",
    "        y_b = model(x[:, :b + 1])\n",
    "        y2[:, b] = y_b[:, b]\n",
    "            \n",
    "    error = ((y1 - y2).norm() / (y1.norm() + y2.norm())).item()\n",
    "    \n",
    "    if error < 1e-5:\n",
    "        print(\"Auto-regressive test passed\")\n",
    "    else:\n",
    "        print(\"Auto-regressive test failed\")\n",
    "        \n",
    "    # print(f\"Error={error}\")\n",
    "    \n",
    "test_auto_regressive(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for evaluating the loss of the PyTorch model on the validation set without defining the model.\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, criterion):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in [\"train\", \"val\"]:\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for batch in generate_batch(split=split):\n",
    "            # Get the inputs and targets\n",
    "            inputs = batch[\"inputs\"]\n",
    "            targets = batch[\"targets\"]\n",
    "            \n",
    "            # Get the model outputs\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Update the total loss and tokens\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += targets.shape[0] * targets.shape[1]\n",
    "        \n",
    "        # Calculate the average loss\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        \n",
    "        # Store the average loss\n",
    "        out[f\"{split}_loss\"] = avg_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscelanea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding_naive(t: int, config=MASTER_CONFIG) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Input: t the position of the token in a sequence\n",
    "    Output: the positional embedding of the position\n",
    "    Parameters: W_p in R^{d_model x l_max} where l_max is the maximum sequence length or context size\n",
    "    Return: e_p in R^{d_model}\n",
    "\n",
    "    Using the sine positional embedding from [VSP17] paper \"Attention is all you need\".\n",
    "    \"\"\"\n",
    "    d_model = config.d_model\n",
    "    l_max = config.l_max\n",
    "    \n",
    "    # Create the positional embedding matrix\n",
    "    W_p = torch.zeros((d_model, l_max))\n",
    "    for t in range(l_max):\n",
    "        for i in range(d_model):\n",
    "            if i % 2 == 0:\n",
    "                W_p[i, t] = np.sin(t / 10000 ** (i / d_model)) # TODO: check if this is correct, or should it be l_max instead of 10000?\n",
    "            else:\n",
    "                W_p[i, t] = np.cos(t / 10000 ** ((i - i % 2) / d_model))\n",
    "    \n",
    "    # Return the positional embedding vector\n",
    "    return W_p[:, t]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
