{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from dataclasses import dataclass, field\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MASTER_CONFIG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lejoon/Projects/cup-transformer/main.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Tokenizer, we want BOS, EOS tokens\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vocab \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39m<PAD>\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m 1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m 2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m 3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m 4\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m 5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m 6\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m 7\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m 8\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m 9\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBall\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mThere are\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m is in\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSwitch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m switches\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m and\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m cup\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m cups\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39msetattr\u001b[39m(MASTER_CONFIG, \u001b[39m\"\u001b[39m\u001b[39mvocab\u001b[39m\u001b[39m\"\u001b[39m, vocab)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39msetattr\u001b[39m(MASTER_CONFIG, \u001b[39m\"\u001b[39m\u001b[39mvocab_size\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlen\u001b[39m(vocab))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m idx_to_s \u001b[39m=\u001b[39m {i:ch \u001b[39mfor\u001b[39;00m i, ch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(vocab)}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MASTER_CONFIG' is not defined"
     ]
    }
   ],
   "source": [
    "# Tokenizer, we want BOS, EOS tokens\n",
    "vocab = ['<PAD>', ' 1', ' 2', ' 3', ' 4', ' 5', ' 6', ' 7', ' 8', ' 9', '\\n', 'Ball', 'There are', ' is in', 'Switch', ' switches', ' of', ' and', ' cup', ' cups']\n",
    "\n",
    "setattr(MASTER_CONFIG, \"vocab\", vocab)\n",
    "setattr(MASTER_CONFIG, \"vocab_size\", len(vocab))\n",
    "\n",
    "idx_to_s = {i:ch for i, ch in enumerate(vocab)}\n",
    "s_to_idx = {ch:i for i, ch in enumerate(vocab)}\n",
    "\n",
    "\n",
    "def encode_tokens(s: str) -> list[int]:\n",
    "    ids = []\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        max_len = -1\n",
    "        max_token = None\n",
    "        for token in s_to_idx.keys():\n",
    "            token_len = len(token)\n",
    "            if s[i:i+token_len] == token:\n",
    "                if token_len > max_len:\n",
    "                    max_len = token_len\n",
    "                    max_token = token\n",
    "        if max_token:\n",
    "            ids.append(s_to_idx[max_token])\n",
    "            i += max_len\n",
    "        else:\n",
    "            print(f\"Unrecognized sequence at index {i}, {s[i:i+1]}\")\n",
    "            \n",
    "            break\n",
    "\n",
    "    return ids\n",
    "\n",
    "def decode_tokens(ids: list[int]) -> str:\n",
    "    return \"\".join([idx_to_s[i] for i in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ball Shuffler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def initial_ball_position(n=3):\n",
    "    return random.randint(1, n)\n",
    "\n",
    "def generate_shuffle_moves(n=3, num_moves=3):\n",
    "    moves = []\n",
    "    \n",
    "    for _ in range(num_moves):\n",
    "        # Randomly pick two different cups\n",
    "        cup1, cup2 = random.sample(range(1, n + 1), 2)\n",
    "        moves.append((cup1, cup2))\n",
    "    \n",
    "    return moves\n",
    "\n",
    "def final_ball_position(initial_position, shuffle_moves):\n",
    "    position = initial_position\n",
    "    for move in shuffle_moves:\n",
    "        # If the ball's current position matches one of the cups in the move, swap it.\n",
    "        if position == move[0]:\n",
    "            position = move[1]\n",
    "        elif position == move[1]:\n",
    "            position = move[0]\n",
    "    \n",
    "    return position\n",
    "    \n",
    "# Generate batches of data\n",
    "def generate_batch_cup_data(n_cups = 3, num_examples=1000, num_moves=3, verbose=False):\n",
    "    inputs_idx = []\n",
    "    targets_idx = []\n",
    "    pad_token = encode_tokens('<PAD>')\n",
    "    pad_token_tensor = torch.tensor(pad_token[0])\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        n_moves = random.choice(range(1,num_moves))\n",
    "\n",
    "        input, target = generate_masked_cup_shuffling_scenario(n_cups=n_cups, n_moves=n_moves)\n",
    "        inputs_idx.append(encode_tokens(input))\n",
    "\n",
    "        # Finds the last cup number in the string\n",
    "        targets_idx.append(encode_tokens(target))\n",
    "    \n",
    "    max_inputs_len = max(len(input) for input in inputs_idx)\n",
    "    \n",
    "    padded_input_ids = [input + pad_token * (max_inputs_len - len(input)) for input in inputs_idx]\n",
    "    target_ids = [targets for targets in targets_idx]\n",
    "        \n",
    "    input_tensor = torch.tensor(padded_input_ids, dtype=torch.long)\n",
    "    target_tensor = torch.tensor(target_ids, dtype=torch.long)\n",
    "\n",
    "    masked_tensor = (input_tensor == pad_token_tensor).long().argmax(dim=1) \n",
    "    \n",
    "    return input_tensor, target_tensor, masked_tensor\n",
    "    \n",
    "def dim(a):\n",
    "    if not type(a) == list:\n",
    "        return []\n",
    "    return [len(a)] + dim(a[0])\n",
    "\n",
    "def dims(a):\n",
    "    for i in range(len(a)):\n",
    "        print(f\"Len of dimension {i}: {len(a[i])}\")\n",
    "\n",
    "def generate_cup_shuffling_scenario(n=3, num_moves=3):\n",
    "    # Generate initial ball position and shuffle movesx\n",
    "    initial_position = initial_ball_position(n)\n",
    "    shuffle_moves = generate_shuffle_moves(n, num_moves)\n",
    "    \n",
    "    # Calculate the final ball position\n",
    "    final_position = final_ball_position(initial_position, shuffle_moves)\n",
    "    \n",
    "    # Construct the input and output strings\n",
    "    string = f\"There are {n} cups and {num_moves} switches of cups\\n\"\n",
    "    string += f\"Ball is in cup {initial_position}\\n\"\n",
    "    string += \"\\n\".join([f\"Switch cup {move[0]} and cup {move[1]}\" for move in shuffle_moves])\n",
    "    string += f\"\\nBall is in cup {final_position}\"\n",
    "    \n",
    "    return string\n",
    "\n",
    "def generate_masked_cup_shuffling_scenario(n_cups=3, n_moves=3):\n",
    "    # Generate initial ball position and shuffle moves\n",
    "    initial_position = initial_ball_position(n_cups)\n",
    "    shuffle_moves = generate_shuffle_moves(n_cups, n_moves)\n",
    "    \n",
    "    # Calculate the final ball position for tokenizing\n",
    "    final_position = \" \" + str(final_ball_position(initial_position, shuffle_moves))\n",
    "    \n",
    "    # Construct the input and output strings\n",
    "    input = f\"There are {n_cups} cups and {n_moves} switches of cups\\n\"\n",
    "    input += f\"Ball is in cup {initial_position}\\n\"\n",
    "    input += \"\\n\".join([f\"Switch cup {move[0]} and cup {move[1]}\" for move in shuffle_moves])\n",
    "    input += f\"\\nBall is in cup<PAD>\"\n",
    "    \n",
    "    return input, final_position\n",
    "\n",
    "class CupDataset(Dataset):\n",
    "    def __init__(self, data, split='train', config=None):\n",
    "        self.inputs, self.targets, self.masked_positions = self._prepare_data(data, split, config)\n",
    "\n",
    "    def _prepare_data(self, data, split, config):\n",
    "        # Assuming generate_batch_cup_data is your data generation function\n",
    "        if split not in [\"train\", \"val\"]:\n",
    "            raise ValueError(\"split must be either 'train' or 'val'\")\n",
    "\n",
    "        # Generate data\n",
    "        generated_data = generate_batch_cup_data(n_cups=config.n_cups, \n",
    "                                                 num_moves=config.n_moves, \n",
    "                                                 num_examples=config.n_samples)\n",
    "       \n",
    "\n",
    "        # Split the data\n",
    "        split_index = int(config.training_split * len(generated_data[0]))\n",
    "        if split == 'train':\n",
    "            split_data = [d[:split_index] for d in generated_data]\n",
    "            print(f\"Splitting data into {len(split_data[0])} training examples\")\n",
    "            return split_data\n",
    "        else:  # split == 'val'\n",
    "            split_data = [d[split_index:] for d in generated_data]\n",
    "            print(f\"Splitting data into {len(split_data[0])} validation examples\")\n",
    "            return split_data\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"inputs\": self.inputs[idx],\n",
    "            \"targets\": self.targets[idx],\n",
    "            \"masked_positions\": self.masked_positions[idx]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[12,  3, 19, 17,  1, 15, 16, 19, 10, 11, 13, 18,  2, 10, 14, 18,  1, 17,\n",
       "          18,  3, 10, 11, 13, 18,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [12,  3, 19, 17,  2, 15, 16, 19, 10, 11, 13, 18,  3, 10, 14, 18,  3, 17,\n",
       "          18,  1, 10, 14, 18,  3, 17, 18,  1, 10, 11, 13, 18,  0],\n",
       "         [12,  3, 19, 17,  2, 15, 16, 19, 10, 11, 13, 18,  2, 10, 14, 18,  1, 17,\n",
       "          18,  3, 10, 14, 18,  3, 17, 18,  2, 10, 11, 13, 18,  0]]),\n",
       " tensor([[2],\n",
       "         [3],\n",
       "         [3]]),\n",
       " tensor([24, 31, 31]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_batch_cup_data(num_examples=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 cups and 5 switches of cups\n",
      "Ball is in cup 4\n",
      "Switch cup 2 and cup 4\n",
      "Switch cup 4 and cup 1\n",
      "Switch cup 3 and cup 2\n",
      "Switch cup 3 and cup 2\n",
      "Switch cup 3 and cup 1\n",
      "Ball is in cup 2\n",
      "The string is 53 tokens long:\n",
      "\n",
      "There are 4 cups and 5 switches of cups\n",
      "Ball is in cup 4\n",
      "Switch cup 2 and cup 4\n",
      "Switch cup 4 and cup 1\n",
      "Switch cup 3 and cup 2\n",
      "Switch cup 3 and cup 2\n",
      "Switch cup 3 and cup 1\n",
      "Ball is in cup 2\n",
      "\n",
      "And has the following tokenization:\n",
      "[12, 4, 19, 17, 5, 15, 16, 19, 10, 11, 13, 18, 4, 10, 14, 18, 2, 17, 18, 4, 10, 14, 18, 4, 17, 18, 1, 10, 14, 18, 3, 17, 18, 2, 10, 14, 18, 3, 17, 18, 2, 10, 14, 18, 3, 17, 18, 1, 10, 11, 13, 18, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s = generate_cup_shuffling_scenario(4, 5)\n",
    "print(s)\n",
    "\n",
    "print(f\"The string is {len(encode_tokens(s))} tokens long:\\n\\n{s}\\n\\nAnd has the following tokenization:\\n{encode_tokens(s)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[(2, 1), (2, 1), (1, 2)]\n",
      "1\n",
      "('There are 3 cups and 3 switches of cups\\nBall is in cup 1\\nSwitch cup 1 and cup 2\\nSwitch cup 2 and cup 3\\nSwitch cup 1 and cup 2\\nBall is in cup<PAD>', ' 3')\n"
     ]
    }
   ],
   "source": [
    "initial_position = initial_ball_position()\n",
    "print(initial_position)\n",
    "\n",
    "shuffle_moves = generate_shuffle_moves()\n",
    "print(shuffle_moves)\n",
    "\n",
    "final_position = final_ball_position(initial_position, shuffle_moves)\n",
    "print(final_position)\n",
    "\n",
    "scenario = generate_masked_cup_shuffling_scenario()\n",
    "print(scenario)\n",
    "\n",
    "\n",
    "#print(generate_cup_data(n_cups=3, num_moves=range(1,5), num_examples=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[12,  4, 19, 17,  3, 15, 16, 19, 10, 11, 13, 18,  4, 10, 14, 18,  4, 17,\n",
      "         18,  3, 10, 14, 18,  3, 17, 18,  2, 10, 14, 18,  2, 17, 18,  1, 10, 11,\n",
      "         13, 18,  0],\n",
      "        [12,  4, 19, 17,  3, 15, 16, 19, 10, 11, 13, 18,  2, 10, 14, 18,  2, 17,\n",
      "         18,  1, 10, 14, 18,  4, 17, 18,  3, 10, 14, 18,  3, 17, 18,  2, 10, 11,\n",
      "         13, 18,  0]]), tensor([[1],\n",
      "        [1]]), tensor([38, 38]))\n"
     ]
    }
   ],
   "source": [
    "print(generate_batch_cup_data(4, 2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models, functions and layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Dummy model for testing purposes. It takes a sequence of tokens and returns a probability distribution over the vocabulary.\n",
    "    \n",
    "    Input: x a sequence of tokens of shape (B, T)\n",
    "    Output: a probability distribution over the vocabulary of shape (B, T, vocab_size)\n",
    "    Parameters: \n",
    "    \"\"\"\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        vocab_size = config.vocab_size\n",
    "        d_model = config.d_model\n",
    "        \n",
    "        # Embedding layer from the vocabulary to the d_model dimension, parameter matrix W_e in R^{vocab_size x d_model}\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # Linear layer from d_model to vocab_size dimension. Parameter matrices W_l1, W_l2 in R^{d_model x d_model}, R^{d_model x vocab_size}\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, vocab_size)\n",
    "        )\n",
    "        \n",
    "        # Returns the number of parameters in the model\n",
    "        self.num_parameters = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        # x is of shape (B, T), each element is an integer in the range [0, vocab_size) representing a token. By seeing each token as a one-hot vector, x can be seen as a tensor of shape (B, T, vocab_size)\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Embedd x into the d_model dimension which makes x of shape (B, T, d_model)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # MLP layer into an unembedding layer, which makes x of shape (B, T, vocab_size)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Softmax over the vocabulary dimension which is the last dimension, returns a probability distribution over the vocabulary, it is of shape (B, T, vocab_size)\n",
    "        logits = F.softmax(x, dim=-1)\n",
    "        \n",
    "        if targets is not None:\n",
    "            # Calculate the loss\n",
    "            \n",
    "            # Flatten the logits and targets to be of shape (B*T, vocab_size) and (B*T) respectively\n",
    "            x = x.view(B*T, self.vocab_size)\n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return loss, logits\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class positional_encoding(nn.Module):\n",
    "    \"\"\" \n",
    "    Positional encoding according to [VSP17] paper \"Attention is all you need\" based on sine and cosine functions.\n",
    "    \n",
    "    B = batch size\n",
    "    T = sequence length\n",
    "    d_model = embedding dimension\n",
    "    \n",
    "    Input: x a sequence of tokens of shape (B, T, d_model)\n",
    "    Output: p, where p is the positional encoding, of shape (B, T, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        d_model = config.d_model\n",
    "        l_max = config.l_max\n",
    "        dtype = config.dtype\n",
    "        \n",
    "        self.p = torch.zeros((1, l_max, d_model)) #TODO: Is this really necessary? Should this be l_max, d_model instead?\n",
    "        num = torch.arange(l_max, dtype=dtype).reshape(-1, 1) # Creates X = [[0], [1], ..., [l_max - 1]]\n",
    "        denum = torch.pow(10000, torch.arange(0, d_model, 2, dtype=dtype) / d_model) # Creates Y = [10000^0/d_model, 10000^2/d_model, ..., 10000^(d_model - 1)/d_model]\n",
    "        self.p[:, :, 0::2] = torch.sin(num / denum)\n",
    "        self.p[:, :, 1::2] = torch.cos(num / denum)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.p[:, :x.shape[1], :].to(x.device)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_q, d_v, d_attn, d_out, bias=False, config=MASTER_CONFIG, mask=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear layers for Q, K, V of dimensions dq, dv, dv respectively\n",
    "        # TODO: More efficient to do the linear transformation together and then split the result?\n",
    "        # TODO: Allow a toggle for whether to use bias or not\n",
    "        self.linear_q = nn.Linear(d_attn, d_q, bias=bias)\n",
    "        self.linear_k = nn.Linear(d_attn, d_v, bias=bias)\n",
    "        self.linear_v = nn.Linear(d_out, d_v, bias=bias)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        _, _, D = x.shape # x is of shape (B, T, d_model) TODO: Maybe should make this dynamic through reshaping -1?\n",
    "        \n",
    "        q = self.linear_q(x) # (B, T, d_model)\n",
    "        k = self.linear_k(x) # (B, T, d_model)\n",
    "        v = self.linear_v(x) # (B, T, d_model)\n",
    "        \n",
    "        S = torch.bmm(q, k.transpose(1, 2)) / np.sqrt(D) # Calculate the attention weights (B, T, d_model) * (B, d_model, T) = (B, T, T)\n",
    "        \n",
    "        if mask == None:\n",
    "            weights = S\n",
    "        else:\n",
    "            weights = S.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        weights = F.softmax(weights, dim=-1) # Apply the softmax on the last dimension, meaning the last dimension sums to 1\n",
    "        v_bar = torch.bmm(weights, v) # Apply the attention weights to the values (B, T, T) * (B, T, d_model) = (B, T, d_model)\n",
    "        # Y_t = att(X_t W_h^Q, X_t W_h^K, X_t W_h^V) = softmax((X_t W_h^Q)(X_t W_h^K)^t / sqrt(d_model)) (X_t W_h^V)\n",
    "\n",
    "        return v_bar\n",
    "        \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, mask=None):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        head_size = d_model // n_heads\n",
    "        \n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(d_attn = head_size, d_out = head_size, d_q=d_model, d_v=d_model, mask=mask) for _ in range(n_heads)]\n",
    "        )\n",
    "        \n",
    "        self.linear_o = nn.Linear(d_model * n_heads, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        H = self.n_heads\n",
    "\n",
    "        B, T, _ = x.shape # x is of shape (B, T, d_model)\n",
    "        \n",
    "        x = x.view(B, T, H, -1) # Reshape x to (B, T, n_heads, d_model/n_heads)\n",
    "        x = x.transpose(1, 2) # Transpose to get shape (B, n_heads, T, d_model/n_heads)\n",
    "        \n",
    "        v = [head(x[:, i, :, :]) for i, head in enumerate(self.heads)] # Apply attention heads to shape (B, 1, T, d_model/n_heads)\n",
    "\n",
    "        v = torch.stack(v, dim=1) # Stack heads to get shape (B, n_heads, T, d_model/n_heads)\n",
    "        v = v.transpose(1, 2).contiguous().view(B, T, -1) # Reshape to (B, T, d_model)\n",
    "        #TODO: Understand what this transformation does\n",
    "        \n",
    "        v_bar = self.linear_o(v) # Apply the linear layer (B, T, d_model) -> (B, T, d_model)\n",
    "        \n",
    "        return v_bar\n",
    "    \n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, mask=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln_mha = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadAttention(d_model=d_model, n_heads=n_heads, mask=mask)\n",
    "        \n",
    "        self.fcn = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is of shape (B, T, d_model)\n",
    "        \n",
    "        x = self.ln_mha(x) # (B, T, d_model) -> (B, T, d_model)\n",
    "        x = self.mha(x) + x # (B, T, d_model) -> (B, T, d_model)\n",
    "        x = self.fcn(x) + x # (B, T, d_model) -> (B, T, d_model)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class cup_GPT(nn.Module):\n",
    "    def __init__(self, config=MASTER_CONFIG):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        self.d_model = config.d_model\n",
    "        self.vocab_size = config.vocab_size  # Define vocab_size here\n",
    "        self.n_layers = config.n_layers\n",
    "        self.l_max = config.l_max\n",
    "        self.causal = config.causal\n",
    "        \n",
    "        d_model = config.d_model\n",
    "        n_heads = config.n_heads\n",
    "        d_model = config.d_model\n",
    "        vocab_size = config.vocab_size\n",
    "        n_layers = config.n_layers\n",
    "        l_max = config.l_max\n",
    "        causal = config.causal\n",
    "        \n",
    "        if causal: #Causal mask\n",
    "            mask = torch.triu(torch.ones((l_max, l_max)), diagonal=1)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_enc = positional_encoding(config)\n",
    "        \n",
    "        self.mha_layers = nn.ModuleList(\n",
    "            [MultiHeadAttentionLayer(d_model=d_model, n_heads=n_heads, mask=mask) for _ in range(n_layers)]\n",
    "        )\n",
    "        \n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.shape # x is of shape (B, T)\n",
    "\n",
    "        x = self.embed(x) # Embed the tokens (B, T) -> (B, T, d_model)\n",
    "\n",
    "        x = x + self.pos_enc(x) # Add the positional encoding (B, T, d_model) -> (B, T, d_model)\n",
    "\n",
    "        for mha_layer in self.mha_layers: \n",
    "            x = mha_layer(x) # Apply the MHA layers (B, T, d_model) -> (B, T, d_model)\n",
    "        \n",
    "        x = self.ln(x) # Apply the layer norm (B, T, d_model) -> (B, T, d_model)\n",
    "        unnorm_logits = self.unembed(x) # Apply the unembedding layer (B, T, d_model) -> (B, T, vocab_size), unnormalized logits\n",
    "        \n",
    "        if targets is not None:            \n",
    "            # Flatten the logits and targets to make the calculation more efficient and return a single value\n",
    "\n",
    "            #unnorm_logits = unnorm_logits.view(B*T, self.vocab_size) # (B, T, vocab_size) -> (B*T, vocab_size)\n",
    "  \n",
    "            #targets = targets.view(B) # (B, T) -> (B*T)\n",
    "            targets = targets.view(B)\n",
    "            # Cross entropy loss in PyTorch takes the unnormalized logits and the targets, so we don't need to softmax the logits.\n",
    "            loss = F.cross_entropy(unnorm_logits, targets, ignore_index = encode_tokens('<PAD>')[0])\n",
    "            \n",
    "        else:\n",
    "            loss = None\n",
    "            \n",
    "        return unnorm_logits, loss\n",
    "    \n",
    "# TODO: Next steps are to implement the decoder part of the transformer.\n",
    "# TODO: Understand how cross entropy loss works in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing batch generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing dummy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized sequence at index 22, 9\n",
      "Unrecognized sequence at index 22, 9\n",
      "Unrecognized sequence at index 22, 1\n",
      "Unrecognized sequence at index 22, 7\n",
      "20\n",
      "tensor([[ 4, 16, 10]]) tensor([[[0.0446, 0.0682, 0.0492, 0.0515, 0.0320, 0.0436, 0.0386, 0.0636,\n",
      "          0.0699, 0.0470, 0.0383, 0.0323, 0.0505, 0.0479, 0.0914, 0.0454,\n",
      "          0.0397, 0.0399, 0.0553, 0.0512],\n",
      "         [0.0426, 0.0493, 0.0804, 0.0472, 0.0510, 0.0570, 0.0570, 0.0430,\n",
      "          0.0586, 0.0440, 0.0472, 0.0471, 0.0474, 0.0603, 0.0369, 0.0426,\n",
      "          0.0528, 0.0460, 0.0490, 0.0406],\n",
      "         [0.0312, 0.0501, 0.0685, 0.0374, 0.0394, 0.0586, 0.0537, 0.0644,\n",
      "          0.0623, 0.0564, 0.0554, 0.0329, 0.0394, 0.0562, 0.0543, 0.0652,\n",
      "          0.0418, 0.0315, 0.0576, 0.0437]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "config = MASTER_CONFIG\n",
    "data = generate_batch_cup_data(config.n_cups, config.n_moves, config.n_samples)\n",
    "\n",
    "model = DummyModel(config)\n",
    "#model.to(config.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "print(MASTER_CONFIG.vocab_size)\n",
    "\n",
    "# Test model\n",
    "x = torch.randint(MASTER_CONFIG.vocab_size, (1, 3))\n",
    "y = model(x)\n",
    "\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon MPS\n",
      "Splitting data into 24000 training examples\n",
      "Splitting data into 6000 validation examples\n"
     ]
    }
   ],
   "source": [
    "# Create a config file for the cup shuffling task\n",
    "@dataclass\n",
    "class MASTER_CONFIG:\n",
    "    # Training\n",
    "    seed: int = 1337\n",
    "    batch_size: int = 32\n",
    "    training_split: float = 0.8\n",
    "    \n",
    "    epochs = 1\n",
    "    batch_eval_internal = 10\n",
    "    learning_rate = 2e-5\n",
    "    eval_iters = 50\n",
    "    \n",
    "    # Model parameters\n",
    "    dtype: torch.dtype = torch.float32\n",
    "    d_model: int = 128 # This is the size of the embedding\n",
    "    l_max: int = 128 # Max sequence length\n",
    "    n_heads: int = 8 # Number of heads in the multi-head attention\n",
    "    n_layers: int = 8 # Number of layers in the transformer of MHA blocks\n",
    "    dropout: float = 0.1 # Dropout rate\n",
    "    causal: bool = True # Whether to use a causal mask in the attention layer\n",
    "    \n",
    "    # Data parameters\n",
    "    n_cups: int = 3\n",
    "    n_moves: int = 4\n",
    "    n_samples: int = 30000\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab: list[str] = field(default_factory = lambda: [])\n",
    "    vocab_size: int = 0\n",
    "    \n",
    "    # Use CUDA or MPS if available else CPU\n",
    "    if (torch.cuda.is_available()):\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA\")\n",
    "    elif (torch.backends.mps.is_available()):\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple Silicon MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "        \n",
    "setattr(MASTER_CONFIG, \"vocab\", vocab)\n",
    "setattr(MASTER_CONFIG, \"vocab_size\", len(vocab))\n",
    "\n",
    "train_dataset = CupDataset(data, split='train', config=MASTER_CONFIG)\n",
    "val_dataset = CupDataset(data, split='val', config=MASTER_CONFIG)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=MASTER_CONFIG.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=MASTER_CONFIG.batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MASTER_CONFIG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lejoon/Projects/cup-transformer/main.ipynb Cell 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Train the model with evaluate_model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_model\u001b[39m(model, optimizer, config\u001b[39m=\u001b[39mMASTER_CONFIG):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# Set the model to train mode\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X26sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     model\u001b[39m.\u001b[39mto(config\u001b[39m.\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MASTER_CONFIG' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Method for evaluating the loss of the PyTorch model on the validation set without defining the model.\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "\n",
    "    for split, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        total_loss = 0.0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for batch in loader:\n",
    "            # Get the inputs and targets\n",
    "            inputs = batch[\"inputs\"]\n",
    "            targets = batch[\"targets\"]\n",
    "            mask_tensor = batch[\"masked_positions\"]\n",
    "\n",
    "            # Get the model outputs\n",
    "            logits, _ = model(inputs)\n",
    "\n",
    "            target_logits = logits[torch.arange(logits.size(0)), mask_tensor]\n",
    "\n",
    "            loss = F.cross_entropy(target_logits, targets.squeeze(1))\n",
    "\n",
    "            # Update the total loss and tokens\n",
    "            total_loss += loss.item() * targets.numel()\n",
    "            total_tokens += targets.numel()\n",
    "\n",
    "        # Calculate the average loss\n",
    "        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "        # Store the average loss\n",
    "        out[split] = avg_loss\n",
    "\n",
    "    model.train()\n",
    "\n",
    "# Train the model with evaluate_model\n",
    "def train_model(model, optimizer, config=MASTER_CONFIG):\n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize the losses\n",
    "    losses = []\n",
    "    \n",
    "    # Initialize the timer\n",
    "    start = time.time()\n",
    "    \n",
    "    # Loop over the training data\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        # Get the inputs and targets\n",
    "        inputs = batch[\"inputs\"]\n",
    "        targets = batch[\"targets\"]\n",
    "        mask_tensor = batch[\"masked_positions\"]\n",
    "        \n",
    "        #Inputs should be the tokens before the masked token, target is in targets\n",
    "        \n",
    "        #Find position of masked token\n",
    "\n",
    "        # Get the model outputs\n",
    "        logits, _ = model(inputs)\n",
    "\n",
    "        target_logits = logits[torch.arange(logits.size(0)), mask_tensor]\n",
    "\n",
    "        loss = F.cross_entropy(target_logits,targets.squeeze(1))\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Append the loss\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Print the loss every 100 iterations\n",
    "        if i % config.batch_size * 10 == 0:\n",
    "            print(f\"Training batch {i}, loss = {loss.item()}\")\n",
    "            \n",
    "        # Evaluate the model every 1000 iterations\n",
    "        if i % config.batch_size * 100 == 0:\n",
    "            print(f\"Evaluating model at batch {i}\")\n",
    "            eval_out = evaluate_model(model)\n",
    "            print(f\"Train loss = {eval_out['train']}, val loss = {eval_out['val']}\")\n",
    "            \n",
    "    # Evaluate the model at the end of training\n",
    "    print(\"Evaluating model...\")\n",
    "    eval_out = evaluate_model(model)\n",
    "    print(f\"Train loss = {eval_out['train']}, val loss = {eval_out['val']}\")\n",
    "    \n",
    "    # Print the total time\n",
    "    print(f\"Total time: {time.time() - start} seconds\")\n",
    "    \n",
    "    # Plot the losses\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data with n_cups=3, n_moves=4, n_samples=30000, batch_size=32, vocab_size=20\n",
      "20\n",
      "tensor([[13, 14, 11]]) (tensor([[[ 0.6928, -0.8957, -0.8119,  0.7585,  0.0682,  0.3508, -0.3231,\n",
      "           0.5230, -0.2699,  0.9262,  0.3870, -1.0316, -1.6305,  0.4778,\n",
      "           0.1935, -0.3910, -1.0256, -1.1440, -0.6965,  0.4925],\n",
      "         [ 0.1962, -0.1202, -0.2957,  0.7718,  0.2603, -0.9040,  0.4544,\n",
      "           0.5387, -0.2896,  0.5046,  0.1836, -1.3450, -2.1036,  0.3174,\n",
      "          -0.2098, -0.2560, -0.9537, -0.3508, -0.1006,  0.8980],\n",
      "         [ 0.9498, -0.0901, -0.8776,  0.4441, -0.0584,  0.0135, -0.1352,\n",
      "           0.1310, -0.3233,  0.4356,  1.0003, -0.7701, -1.2199,  0.4071,\n",
      "          -0.2606, -1.0936, -1.3642, -0.6736, -0.7485,  0.8523]]],\n",
      "       grad_fn=<ViewBackward0>), None)\n"
     ]
    }
   ],
   "source": [
    "config = MASTER_CONFIG\n",
    "print(f'Generating data with n_cups={config.n_cups}, n_moves={config.n_moves}, n_samples={config.n_samples}, batch_size={config.batch_size}, vocab_size={config.vocab_size}')\n",
    "data = generate_batch_cup_data(n_cups=config.n_cups, num_moves=config.n_moves, num_examples=config.n_samples)\n",
    "\n",
    "model = cup_GPT(config)\n",
    "#model.to(config.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "pad_token = encode_tokens('<PAD>')[0]\n",
    "\n",
    "print(MASTER_CONFIG.vocab_size)\n",
    "\n",
    "# Test model\n",
    "x = torch.randint(MASTER_CONFIG.vocab_size, (1, 3))\n",
    "y = model(x)\n",
    "\n",
    "print(x, y)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lejoon/Projects/cup-transformer/main.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train_model(model, optimizer, config\u001b[39m=\u001b[39mMASTER_CONFIG)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, config=MASTER_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 cups and 4 switches of cups\n",
      "Ball is in cup 3\n",
      "Switch cup 2 and cup 1\n",
      "Switch cup 3 and cup 2\n",
      "Switch cup 1 and cup 3\n",
      "Switch cup 1 and cup 3\n",
      "Ball is in cup<PAD>  2\n",
      "torch.Size([1, 45, 20])\n",
      "tensor([-1.1124,  3.3370,  3.4480,  3.5079, -1.3007, -1.4559, -1.6767, -1.2643,\n",
      "        -0.7619, -1.2158, -1.8534, -1.1791, -1.2965, -1.2714, -1.8033, -1.6372,\n",
      "        -1.4643, -1.5011, -1.0022, -1.5190], grad_fn=<SliceBackward0>)\n",
      "Token: <PAD>, probability: 0.003374143736436963\n",
      "Token:  1, probability: 0.2887522280216217\n",
      "Token:  2, probability: 0.32265523076057434\n",
      "Token:  3, probability: 0.3425672948360443\n",
      "Token:  4, probability: 0.002795099513605237\n",
      "Token:  5, probability: 0.0023931728210300207\n",
      "Token:  6, probability: 0.0019190349848940969\n",
      "Token:  7, probability: 0.002898640464991331\n",
      "Token:  8, probability: 0.004790663253515959\n",
      "Token:  9, probability: 0.003042827360332012\n",
      "Token: \n",
      ", probability: 0.0016082323854789138\n",
      "Token: Ball, probability: 0.003156426828354597\n",
      "Token: There are, probability: 0.0028067580424249172\n",
      "Token:  is in, probability: 0.0028782575391232967\n",
      "Token: Switch, probability: 0.0016908288234844804\n",
      "Token:  switches, probability: 0.0019963153172284365\n",
      "Token:  of, probability: 0.002373209921643138\n",
      "Token:  and, probability: 0.0022875526919960976\n",
      "Token:  cup, probability: 0.00376722845248878\n",
      "Token:  cups, probability: 0.0022467898670583963\n",
      "3\n",
      "Predicted token:  3, actual token:  2\n"
     ]
    }
   ],
   "source": [
    "# make a function out of this and calculate it several times to get the average error\n",
    "\n",
    "\n",
    "model.eval()\n",
    "# Generate a single example\n",
    "input, final_position = generate_masked_cup_shuffling_scenario(n_cups=MASTER_CONFIG.n_cups, n_moves=MASTER_CONFIG.n_moves)\n",
    "print(input, final_position)\n",
    "\n",
    "# Tokenize the input\n",
    "input_tokens = encode_tokens(input)\n",
    "# Remove the last token\n",
    "input_tokens = input_tokens[:-1]\n",
    "# Convert to a tensor\n",
    "input_tensor = torch.tensor(input_tokens, dtype=torch.long)\n",
    "# Add a batch dimension\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "# Feed the input to the model\n",
    "logits, _ = model(input_tensor)\n",
    "print(logits.shape)\n",
    "# Get the last logits\n",
    "last_logits = logits[0, -1, :]\n",
    "print(last_logits)\n",
    "# show probabilities of different tokens, by printing the token and the probability next to each other\n",
    "probs = F.softmax(last_logits, dim=-1)\n",
    "for i in range(len(last_logits)):\n",
    "    print(f\"Token: {idx_to_s[i]}, probability: {probs[i].item()}\")\n",
    "    \n",
    "# Get the predicted token\n",
    "predicted_token = torch.argmax(last_logits).item()\n",
    "print(predicted_token)\n",
    "# Get the actual token\n",
    "actual_token = encode_tokens(final_position)[0]\n",
    "# Print the predicted and actual tokens\n",
    "print(f\"Predicted token: {idx_to_s[predicted_token]}, actual token: {idx_to_s[actual_token]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros_like(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lejoon/Projects/cup-transformer/main.ipynb Cell 26\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39mprint\u001b[39m(error)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X34sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m# print(f\"Error={error}\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X34sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m test_auto_regressive(model)\n",
      "\u001b[1;32m/Users/lejoon/Projects/cup-transformer/main.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(config\u001b[39m.\u001b[39mvocab_size, (\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y1 \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X34sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m y2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros_like(y1)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(x\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lejoon/Projects/cup-transformer/main.ipynb#X34sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     y_b \u001b[39m=\u001b[39m model(x[:, :b \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: zeros_like(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "def test_auto_regressive(model, config=MASTER_CONFIG):\n",
    "    '''\n",
    "    Tests if the model is auto-regressive by comparing the output of the model when given the entire input sequence and when given the input sequence one token at a time.\n",
    "    '''\n",
    "    # TODO: Not sure this works.\n",
    "    # shape vocab_size, batch_size, block_size\n",
    "    x = torch.randint(config.vocab_size, (1, 3))\n",
    "    y1 = model(x)\n",
    "    \n",
    "    y2 = torch.zeros_like(y1)\n",
    "    for b in range(x.size(1)):\n",
    "        y_b = model(x[:, :b + 1])\n",
    "        y2[:, b] = y_b[:, b]\n",
    "            \n",
    "    error = ((y1 - y2).norm() / (y1.norm() + y2.norm())).item()\n",
    "    \n",
    "    if error < 1e-5:\n",
    "        print(\"Auto-regressive test passed\")\n",
    "        print(error)\n",
    "    else:\n",
    "        print(\"Auto-regressive test failed\")\n",
    "        print(error)\n",
    "        \n",
    "    # print(f\"Error={error}\")\n",
    "    \n",
    "test_auto_regressive(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miscelanea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_embedding_naive(t: int, config=MASTER_CONFIG) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Input: t the position of the token in a sequence\n",
    "    Output: the positional embedding of the position\n",
    "    Parameters: W_p in R^{d_model x l_max} where l_max is the maximum sequence length or context size\n",
    "    Return: e_p in R^{d_model}\n",
    "\n",
    "    Using the sine positional embedding from [VSP17] paper \"Attention is all you need\".\n",
    "    \"\"\"\n",
    "    d_model = config.d_model\n",
    "    l_max = config.l_max\n",
    "    \n",
    "    # Create the positional embedding matrix\n",
    "    W_p = torch.zeros((d_model, l_max))\n",
    "    for t in range(l_max):\n",
    "        for i in range(d_model):\n",
    "            if i % 2 == 0:\n",
    "                W_p[i, t] = np.sin(t / 10000 ** (i / d_model)) # TODO: check if this is correct, or should it be l_max instead of 10000?\n",
    "            else:\n",
    "                W_p[i, t] = np.cos(t / 10000 ** ((i - i % 2) / d_model))\n",
    "    \n",
    "    # Return the positional embedding vector\n",
    "    return W_p[:, t]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
