{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ball Shuffler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[(2, 3), (2, 3), (2, 3)]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def initial_ball_position(n=3):\n",
    "    return random.randint(1, n)\n",
    "\n",
    "initial_position = initial_ball_position()\n",
    "print(initial_position)\n",
    "\n",
    "def generate_shuffle_moves(n=3, num_moves=3):\n",
    "    moves = []\n",
    "    \n",
    "    for _ in range(num_moves):\n",
    "        # Randomly pick two different cups\n",
    "        cup1, cup2 = random.sample(range(1, n + 1), 2)\n",
    "        moves.append((cup1, cup2))\n",
    "    \n",
    "    return moves\n",
    "\n",
    "shuffle_moves = generate_shuffle_moves()\n",
    "print(shuffle_moves)\n",
    "\n",
    "def final_ball_position(initial_position, shuffle_moves):\n",
    "    position = initial_position\n",
    "    for move in shuffle_moves:\n",
    "        # If the ball's current position matches one of the cups in the move, swap it.\n",
    "        if position == move[0]:\n",
    "            position = move[1]\n",
    "        elif position == move[1]:\n",
    "            position = move[0]\n",
    "    \n",
    "    return position\n",
    "\n",
    "final_position = final_ball_position(initial_position, shuffle_moves)\n",
    "print(final_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ball is in cup 1\n",
      "Switch cup 2 and cup 1\n",
      "Switch cup 2 and cup 1\n",
      "Switch cup 1 and cup 3\n",
      "Ball is in cup 3\n",
      "[('Ball is in cup 2\\nSwitch cup 3 and cup 1\\nSwitch cup 2 and cup 3\\nSwitch cup 1 and cup 2', 'Ball is in cup 3'), ('Ball is in cup 1\\nSwitch cup 3 and cup 2\\nSwitch cup 2 and cup 3\\nSwitch cup 3 and cup 2', 'Ball is in cup 1')]\n"
     ]
    }
   ],
   "source": [
    "def generate_cup_shuffling_scenario(n=3, num_moves=3):\n",
    "    # Generate initial ball position and shuffle moves\n",
    "    initial_position = initial_ball_position(n)\n",
    "    shuffle_moves = generate_shuffle_moves(n, num_moves)\n",
    "    \n",
    "    # Calculate the final ball position\n",
    "    final_position = final_ball_position(initial_position, shuffle_moves)\n",
    "    \n",
    "    # Construct the input and output strings\n",
    "    input_str = f\"Ball is in cup {initial_position}\\n\"\n",
    "    input_str += \"\\n\".join([f\"Switch cup {move[0]} and cup {move[1]}\" for move in shuffle_moves])\n",
    "    \n",
    "    output_str = f\"Ball is in cup {final_position}\"\n",
    "    \n",
    "    return input_str, output_str\n",
    "\n",
    "input_scenario, output_scenario = generate_cup_shuffling_scenario()\n",
    "print(input_scenario)\n",
    "print(output_scenario)\n",
    "input_scenario\n",
    "\n",
    "# Generate batches of data\n",
    "def generate_cup_data(n=3, num_moves=3, num_examples=10):\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_examples):\n",
    "        input_str, output_str = generate_cup_shuffling_scenario(n, num_moves)\n",
    "        data.append((input_str, output_str))\n",
    "    \n",
    "    return data\n",
    "\n",
    "print(generate_cup_data(n=3, num_moves=3, num_examples=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch cup 2 and cup 1\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer, we want BOS, EOS tokens\n",
    "vocab = ['<BOS>', '<EOS>', '\\n', 'Ball', ' is in ', 'Switch', ' and', ' cup 1', ' cup 2', ' cup 3', ' cup 4', ' cup 5', ' cup 6', ' cup 7', ' cup 8', ' cup 9']\n",
    "\n",
    "idx_to_s = {i:ch for i, ch in enumerate(vocab)}\n",
    "s_to_idx = {ch:i for i, ch in enumerate(vocab)}\n",
    "\n",
    "def encode(s):\n",
    "    ids = []\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        max_len = -1\n",
    "        max_token = None\n",
    "        for token in s_to_idx.keys():\n",
    "            token_len = len(token)\n",
    "            if s[i:i+token_len] == token:\n",
    "                if token_len > max_len:\n",
    "                    max_len = token_len\n",
    "                    max_token = token\n",
    "        if max_token:\n",
    "            ids.append(s_to_idx[max_token])\n",
    "            i += max_len\n",
    "        else:\n",
    "            print(f\"Unrecognized sequence at index {i}, {s[i:i+1]}\")\n",
    "            \n",
    "            break\n",
    "    return ids\n",
    "\n",
    "def decode(ids):\n",
    "    return \"\".join([idx_to_s[i] for i in ids])\n",
    "\n",
    "#s = \"Switch cup 2 and cup 1\"\n",
    "#print(decode(encode(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon MPS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a config file for the cup shuffling task\n",
    "@dataclass\n",
    "class MASTER_CONFIG:\n",
    "    seed: int = 1337\n",
    "    batch_size: int = 32\n",
    "    training_split: float = 0.8\n",
    "    \n",
    "    dtype: torch.dtype = torch.float32\n",
    "    d_model: int = 128\n",
    "    \n",
    "    n_cups: int = 3\n",
    "    n_moves: int = 3\n",
    "    n_samples: int = 1000\n",
    "    tokens: dict = field(default_factory=lambda: tokens)\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    n_embed: int = 1024\n",
    "    l_max: int = 32\n",
    "    n_heads: int = 8\n",
    "    head_size: int = 128 # n_embed/n_heads?\n",
    "    n_layers: int = 8\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    \n",
    "    max_iters = 100\n",
    "    eval_interval = 10\n",
    "    learning_rate = 2e-5\n",
    "    \n",
    "    # Use CUDA or MPS if available else CPU\n",
    "    if (torch.cuda.is_available()):\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using CUDA\")\n",
    "    elif (torch.backends.mps.is_available()):\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"Using Apple Silicon MPS\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    eval_iters = 50\n",
    "\n",
    "\n",
    "\n",
    "# Method for generating data and labels for batches.\n",
    "def generate_batch(data, split: str, config=MASTER_CONFIG()):\n",
    "    B = config.batch_size\n",
    "    T = config.l_max\n",
    "    vocab_size = config.vocab_size\n",
    "    \n",
    "    for \n",
    "    \n",
    "def positional_embedding_naive(t: int, config=MASTER_CONFIG()) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Input: t the position of the token in a sequence\n",
    "    Output: the positional embedding of the position\n",
    "    Parameters: W_p in R^{d_model x l_max} where l_max is the maximum sequence length or context size\n",
    "    Return: e_p in R^{d_model}\n",
    "\n",
    "    Using the sine positional embedding from [VSP17] paper \"Attention is all you need\".\n",
    "    \"\"\"\n",
    "    d_model = config.d_model\n",
    "    l_max = config.l_max\n",
    "    \n",
    "    # Create the positional embedding matrix\n",
    "    W_p = torch.zeros((d_model, l_max))\n",
    "    for t in range(l_max):\n",
    "        for i in range(d_model):\n",
    "            if i % 2 == 0:\n",
    "                W_p[i, t] = np.sin(t / 10000 ** (i / d_model)) # TODO: check if this is correct, or should it be l_max instead of 10000?\n",
    "            else:\n",
    "                W_p[i, t] = np.cos(t / 10000 ** ((i - i % 2) / d_model))\n",
    "    \n",
    "    # Return the positional embedding vector\n",
    "    return W_p[:, t]\n",
    "\n",
    "class positional_encoding(nn.Module):\n",
    "    \"\"\" \n",
    "    Positional encoding according to [VSP17] paper \"Attention is all you need\" based on sine and cosine functions.\n",
    "    \n",
    "    Input: x a sequence of tokens of shape (B, T, d_model)\n",
    "    Output: x + p, where p is the positional encoding, of shape (B, T, d_model)\n",
    "    \"\"\"\n",
    "    def __init__(self, config=MASTER_CONFIG()):\n",
    "        super().__init__()\n",
    "        d_model = config.d_model\n",
    "        l_max = config.l_max\n",
    "        dtype = config.dtype\n",
    "        \n",
    "        self.p = torch.zeros((1, d_model, l_max))\n",
    "        \n",
    "        # Creates X = [[0], [1], ..., [l_max - 1]]\n",
    "        num = torch.arange(l_max, dtype=dtype).reshape(-1, 1)\n",
    "        # Creates Y = [10000^0/d_model, 10000^2/d_model, ..., 10000^(d_model - 1)/d_model]\n",
    "        denum = torch.pow(10000, torch.arange(0, d_model, 2, dtype=dtype) / d_model)\n",
    "        \n",
    "        self.p[:, :, 0::2] = torch.sin(num / denum)\n",
    "        self.p[:, :, 1::2] = torch.cos(num / denum)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.p[:, :x.shape[1], :].to(x.device)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "def masked_softmask(x: torch.Tensor, mask: torch.Tensor, value: float = 1e6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Softmax with masking on the last dimension. Replaces the masked values with value.\n",
    "    param x: input tensor of (B, T, C) dimension\n",
    "    param mask: mask tensor of (B) or (B, T) dimension\n",
    "    param dim: dimension to apply softmax, default is -1 which is the last dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    #x = x.masked_fill(~mask, float('-inf'))\n",
    "    #x = F.softmax(x, dim=dim)\n",
    "    #return x\n",
    "    \n",
    "    # Return dummy until finished\n",
    "    return x\n",
    "\n",
    "    \n",
    " \n",
    "class DummyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Dummy model for testing purposes. It takes a sequence of tokens and returns a probability distribution over the vocabulary.\n",
    "    \n",
    "    Input: x a sequence of tokens of shape (B, T)\n",
    "    Output: a probability distribution over the vocabulary of shape (B, T, vocab_size)\n",
    "    Parameters: \n",
    "    \"\"\"\n",
    "    def __init__(self, config=MASTER_CONFIG()):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        vocab_size = config.vocab_size\n",
    "        d_model = config.d_model\n",
    "        \n",
    "        # Embedding layer from the vocabulary to the d_model dimension, parameter matrix W_e in R^{vocab_size x d_model}\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # Linear layer from d_model to vocab_size dimension. Parameter matrices W_l1, W_l2 in R^{d_model x d_model}, R^{d_model x vocab_size}\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, vocab_size)\n",
    "        )\n",
    "        \n",
    "        # Returns the number of parameters in the model\n",
    "        self.num_parameters = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        # x is of shape (B, T), each element is an integer in the range [0, vocab_size) representing a token. By seeing each token as a one-hot vector, x can be seen as a tensor of shape (B, T, vocab_size)\n",
    "        B, T = x.shape\n",
    "        \n",
    "        # Embedd x into the d_model dimension which makes x of shape (B, T, d_model)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # MLP layer into an unembedding layer, which makes x of shape (B, T, vocab_size)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Softmax over the vocabulary dimension which is the last dimension, returns a probability distribution over the vocabulary, it is of shape (B, T, vocab_size)\n",
    "        logits = F.softmax(x, dim=-1)\n",
    "        \n",
    "        if targets is not None:\n",
    "            # Calculate the loss\n",
    "            \n",
    "            # Flatten the logits and targets to be of shape (B*T, vocab_size) and (B*T) respectively\n",
    "            x = x.view(B*T, self.vocab_size)\n",
    "            targets = targets.view(B*T)\n",
    "            \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            return loss, logits\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing dummy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3]) torch.Size([1, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "config = MASTER_CONFIG()\n",
    "data = generate_cup_data(config.n_cups, config.n_moves, config.n_samples)\n",
    "\n",
    "model = DummyModel(config)\n",
    "#model.to(config.device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "# Test model\n",
    "x = torch.randint(MASTER_CONFIG().vocab_size, (1, 3))\n",
    "y = model(x)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-regressive test passed\n"
     ]
    }
   ],
   "source": [
    "def test_auto_regressive(model, config=MASTER_CONFIG()):\n",
    "    '''\n",
    "    Tests if the model is auto-regressive by comparing the output of the model when given the entire input sequence and when given the input sequence one token at a time.\n",
    "    '''\n",
    "    \n",
    "    # shape vocab_size, batch_size, block_size\n",
    "    x = torch.randint(config.vocab_size, (1, 3))\n",
    "    y1 = model(x)\n",
    "    \n",
    "    y2 = torch.zeros_like(y1)\n",
    "    for b in range(x.size(1)):\n",
    "        y_b = model(x[:, :b + 1])\n",
    "        y2[:, b] = y_b[:, b]\n",
    "            \n",
    "    error = ((y1 - y2).norm() / (y1.norm() + y2.norm())).item()\n",
    "    \n",
    "    if error < 1e-5:\n",
    "        print(\"Auto-regressive test passed\")\n",
    "    else:\n",
    "        print(\"Auto-regressive test failed\")\n",
    "        \n",
    "    # print(f\"Error={error}\")\n",
    "    \n",
    "test_auto_regressive(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method for evaluating the loss of the PyTorch model on the validation set without defining the model.\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, criterion):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split in [\"train\", \"val\"]:\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for batch in generate_batch(split=split):\n",
    "            # Get the inputs and targets\n",
    "            inputs = batch[\"inputs\"]\n",
    "            targets = batch[\"targets\"]\n",
    "            \n",
    "            # Get the model outputs\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Update the total loss and tokens\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += targets.shape[0] * targets.shape[1]\n",
    "        \n",
    "        # Calculate the average loss\n",
    "        avg_loss = total_loss / total_tokens\n",
    "        \n",
    "        # Store the average loss\n",
    "        out[f\"{split}_loss\"] = avg_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
